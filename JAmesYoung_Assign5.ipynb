{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFS 774 Assignment 5: Machine Learning Library (MLlib)\n",
    "\n",
    "Now you should have known that Spark has three different data structures available through its APIs: RDD, Dataframe (that is similar to but different from Pandas or R dataframe), and Dataset (In spark 2.0, Dataframe and Dataset APIs are unified. However, Dataset is only avaible for Spark Scala or Java API and is not available for Python). \n",
    "\n",
    "Spark Machine Learning Library includes two major packages for doing machine learning: SparkML and SparkMLLib. SparkMLLib is used with RDDs, while SparkML supports Dataframes. In this assignment, you need to use the SparkML package rather than SparkMLLib and store your datasets as spark Dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case description\n",
    "\n",
    "A supermarket is offering a new line of organic products. The supermarket's management wants to determine which customers are likely to purchase these products.\n",
    "The supermarket has a customer loyalty program. As an initial buyer incentive plan, the supermarket provided coupons for the organic products to all of the loyalty program participants and collected data that includes whether these customers purchased any of the organic products.\n",
    "The ORGANICS data set contains 13 variables and 22222 observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Description\n",
    "The variables in the data set are shown below with the appropriate roles and levels:\n",
    "![Organics description](organics.png)\n",
    "The variables in this dataset has been classified into two categories: interval (i.e., real-valued) vs. nominal (i.e., categorical). Please note in this dataset, you have two dependent variables: TargetBuy (a binary variable) and TargetAmt (an interval variable). You need to deal with the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The overall goal of this assignment is to do a classification. The dependent variable is \"TargetBuy\". We want to predict whether a customer will buy organic food or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises will include an explanation of what is expected, followed by code cells where one cell will have one or more `<FILL IN>` sections.  The cell that needs to be modified will have `# TODO: Replace <FILL IN> with appropriate code`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+----------+---------------+---------+----------+------------+---------+---------+--------+---------+---------+\n",
      "|   ID|DemAffl|DemAge|DemCluster|DemClusterGroup|DemGender|    DemReg|    DemTVReg|PromClass|PromSpend|PromTime|TargetBuy|TargetAmt|\n",
      "+-----+-------+------+----------+---------------+---------+----------+------------+---------+---------+--------+---------+---------+\n",
      "|  140|     10|    76|        16|              C|        U|  Midlands|Wales & West|     Gold|  16000.0|       4|        0|        0|\n",
      "|  620|      4|    49|        35|              D|        U|  Midlands|Wales & West|     Gold|   6000.0|       5|        0|        0|\n",
      "|  868|      5|    70|        27|              D|        F|  Midlands|Wales & West|   Silver|     0.02|       8|        1|        1|\n",
      "| 1120|     10|    65|        51|              F|        M|  Midlands|    Midlands|      Tin|     0.01|       7|        1|        1|\n",
      "| 2313|     11|    68|         4|              A|        F|  Midlands|    Midlands|      Tin|     0.01|       8|        0|        0|\n",
      "| 2771|      9|    72|        28|              D|        U|     North|      N West| Platinum| 20759.81|       3|        0|        0|\n",
      "| 3131|     11|    74|         3|              A|        F|  Midlands|        East|      Tin|     0.01|       8|        0|        0|\n",
      "| 3328|     13|    62|        32|              D|        M|     North|      N East|      Tin|     0.01|       5|        0|        0|\n",
      "| 4529|     10|    62|        49|              F|        M|  Midlands|        East|   Silver|  2038.76|       3|        0|        0|\n",
      "| 5886|     14|    43|        49|              F|        F|      null|        null|     Gold|   6000.0|       1|        1|        1|\n",
      "| 7420|      7|    60|        52|              F|        F|     North|      N East|     Gold|  11000.0|       2|        0|        0|\n",
      "| 9814|      5|  null|        24|              C|        M|South East|      London|   Silver|   5000.0|       1|        1|        1|\n",
      "|10006|      9|    51|        52|              F|        F|  Midlands|    Midlands|   Silver|    300.0|      11|        0|        0|\n",
      "|10219|      6|    64|        19|              C|        F|South East|  S & S East|      Tin|     0.01|       9|        0|        0|\n",
      "|10812|     16|    37|        18|              C|        F|South East|      London|      Tin|     0.01|       4|        1|        2|\n",
      "|11207|      8|    54|        28|              D|        M|  Midlands|    Midlands|   Silver|   1420.0|       1|        0|        0|\n",
      "|11932|      5|    70|        15|              B|        F|  Midlands|    Midlands|     Gold|  6104.66|       8|        0|        0|\n",
      "|14656|   null|    42|        19|              C|        F|  Midlands|        East|      Tin|     0.01|       5|        1|        1|\n",
      "|15350|      7|  null|        40|              E|        F|  Scottish|  C Scotland|      Tin|     0.01|       5|        1|        1|\n",
      "|17302|      7|    49|        28|              D|     null|South East|      London|      Tin|     0.01|       7|        0|        0|\n",
      "+-----+-------+------+----------+---------------+---------+----------+------------+---------+---------+--------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = spark.read.csv('organics.csv', inferSchema=True, header=True) #<FILL IN> # read csv data, set inferSchema to be true.\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ID column is not needed for machine learning. The variable DemCluster is a nominal variable that includes too many categories, and we will remove this variable. We also want to remove the dependent variable \"TargetAmt\" we will not use. In the cell below, please write code to 1) filter out the unneeded columns including 'TargetAmt', 'ID' and 'DemCluster', and 2) change the datatype of the interval independent variables including 'DemAffl', 'DemAge', and 'PromTime' from interger into double for convenient data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- DemAffl: integer (nullable = true)\n",
      " |-- DemAge: integer (nullable = true)\n",
      " |-- DemClusterGroup: string (nullable = true)\n",
      " |-- DemGender: string (nullable = true)\n",
      " |-- DemReg: string (nullable = true)\n",
      " |-- DemTVReg: string (nullable = true)\n",
      " |-- PromClass: string (nullable = true)\n",
      " |-- PromSpend: double (nullable = true)\n",
      " |-- PromTime: integer (nullable = true)\n",
      " |-- TargetBuy: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- DemAffl: double (nullable = true)\n",
      " |-- DemAge: double (nullable = true)\n",
      " |-- DemClusterGroup: string (nullable = true)\n",
      " |-- DemGender: string (nullable = true)\n",
      " |-- DemReg: string (nullable = true)\n",
      " |-- DemTVReg: string (nullable = true)\n",
      " |-- PromClass: string (nullable = true)\n",
      " |-- PromSpend: double (nullable = true)\n",
      " |-- PromTime: double (nullable = true)\n",
      " |-- TargetBuy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 1. drop two columns ID, DemCluster, and TargetAmt\n",
    "data = data.drop('DemCluster', 'TargetAmt') # please use the method drop to drop the columns.\n",
    "data.printSchema()\n",
    "data.head(3) # this is an action. Head() returns a list of rows.\n",
    "from pyspark.sql.types import DoubleType\n",
    "# step 2. change the data types of  'DemAffl', 'DemAge', and 'PromTime' to double. Please refer to https://stackoverflow.com/questions/32284620/how-to-change-a-dataframe-column-from-string-type-to-double-type-in-pyspark to learn how to change the datatype of a variable\n",
    "for variable in ['DemAffl', 'DemAge','PromTime']:\n",
    "    data = data.withColumn(variable, data[variable].cast(DoubleType()))\n",
    "    \n",
    "\n",
    "# print schema again to verify if the data type conversion works. \n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the cell below, write code to use the describe function to show basic stats only for the interval variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+-----------------+\n",
      "|summary|          DemAffl|            DemAge|        PromSpend|         PromTime|\n",
      "+-------+-----------------+------------------+-----------------+-----------------+\n",
      "|  count|            21137|             20713|            22222|            21941|\n",
      "|   mean|8.711832331929791|53.796890841500506|4420.248964089997|6.564605077252632|\n",
      "| stddev|3.421194092206597|13.205780845120188|7559.046597013129|4.657208722596065|\n",
      "|    min|              0.0|              18.0|             0.01|              0.0|\n",
      "|    max|             34.0|              79.0|        296313.85|             39.0|\n",
      "+-------+-----------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interval_variables = ['DemAffl','DemAge', 'PromSpend','PromTime']\n",
    "data.describe(interval_variables).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the box below, please define a list that contains the names of the interval variables that have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "#data.len().show()\n",
    "\n",
    "interval_with_missing = ['DemAffl','DemAge','PromTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the box below, please write code to show the value count of each nominal(categorical) variable including 'TargetBuy'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|DemClusterGroup|count|\n",
      "+---------------+-----+\n",
      "|              F| 3949|\n",
      "|           null|  674|\n",
      "|              E| 2607|\n",
      "|              B| 4144|\n",
      "|              U|   54|\n",
      "|              D| 4378|\n",
      "|              C| 4566|\n",
      "|              A| 1850|\n",
      "+---------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|DemGender|count|\n",
      "+---------+-----+\n",
      "|        F|12148|\n",
      "|     null| 2513|\n",
      "|        M| 5815|\n",
      "|        U| 1746|\n",
      "+---------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|    DemReg|count|\n",
      "+----------+-----+\n",
      "|  Scottish| 1368|\n",
      "|      null|  466|\n",
      "|South East| 8634|\n",
      "|South West|  691|\n",
      "|  Midlands| 6740|\n",
      "|     North| 4323|\n",
      "+----------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|    DemTVReg|count|\n",
      "+------------+-----+\n",
      "|      Ulster|  266|\n",
      "|      N East|  785|\n",
      "|      Border|  203|\n",
      "|      S West|  691|\n",
      "|        null|  465|\n",
      "|      London| 6189|\n",
      "|   Yorkshire| 1443|\n",
      "|        East| 1649|\n",
      "|      N Scot|  329|\n",
      "|      N West| 2096|\n",
      "|  C Scotland|  836|\n",
      "|    Midlands| 3122|\n",
      "|Wales & West| 1703|\n",
      "|  S & S East| 2445|\n",
      "+------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|PromClass|count|\n",
      "+---------+-----+\n",
      "|      Tin| 6487|\n",
      "| Platinum|  840|\n",
      "|   Silver| 8572|\n",
      "|     Gold| 6323|\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|TargetBuy|count|\n",
      "+---------+-----+\n",
      "|        1| 5504|\n",
      "|        0|16718|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nominal_with_dependent = ['DemClusterGroup', 'DemGender', 'DemReg', 'DemTVReg', 'PromClass', 'TargetBuy']\n",
    "for column in nominal_with_dependent:\n",
    "    data.groupBy(column).count().show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the box below, please define a list that contains the names of the nominal variables that have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_with_missing = ['DemClusterGroup', 'DemGender', 'DemReg', 'DemTVReg']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Missing value imputation for interval variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If a continuous variable contains missing values, we do missing value imputation in two steps: 1) We add a missing value indicator; and 2) we replace missing values with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|DemAfflMissing|count|\n",
      "+--------------+-----+\n",
      "|             1| 1085|\n",
      "|             0|21137|\n",
      "+--------------+-----+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|    DemAfflImputed|\n",
      "+-------+------------------+\n",
      "|  count|             22222|\n",
      "|   mean| 8.711832331929804|\n",
      "| stddev|3.3366243422702384|\n",
      "|    min|               0.0|\n",
      "|    max|              34.0|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------------+-----+\n",
      "|DemAgeMissing|count|\n",
      "+-------------+-----+\n",
      "|            1| 1509|\n",
      "|            0|20713|\n",
      "+-------------+-----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|    DemAgeImputed|\n",
      "+-------+-----------------+\n",
      "|  count|            22222|\n",
      "|   mean|53.79689084149959|\n",
      "| stddev|12.74950444653274|\n",
      "|    min|             18.0|\n",
      "|    max|             79.0|\n",
      "+-------+-----------------+\n",
      "\n",
      "+---------------+-----+\n",
      "|PromTimeMissing|count|\n",
      "+---------------+-----+\n",
      "|              1|  281|\n",
      "|              0|21941|\n",
      "+---------------+-----+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|  PromTimeImputed|\n",
      "+-------+-----------------+\n",
      "|  count|            22222|\n",
      "|   mean|6.564605077252632|\n",
      "| stddev|4.627668213674665|\n",
      "|    min|              0.0|\n",
      "|    max|             39.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.feature import Imputer\n",
    "for variable in interval_with_missing:\n",
    "    # step 1. add a missing indicator: \n",
    "    indicator_name = variable+\"Missing\" # The variable name of the missing value indictor \n",
    "    data = data.withColumn(indicator_name, f.when(f.isnull(data[variable]), 1).otherwise(0))\n",
    "    # do value count to verify.\n",
    "    data.groupBy(indicator_name).count().show()\n",
    "    # step 2: replace missing values with the mean\n",
    "    imputed_name = variable + \"Imputed\" # The variable name of the imputed variable\n",
    "    imputer = Imputer(strategy='mean', inputCols=[variable], outputCols=[imputed_name])\n",
    "    imputer_model =imputer.fit(data)\n",
    "    data = imputer_model.transform(data)\n",
    "    # run the describe to check whether the missing values has been imputed. The count of the imputed variables should be 22222.\n",
    "    data.describe(imputed_name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Missing value imputation for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We replace missing values with the string 'unknown'; however, in our dataset, the DemGender column contains \"M\", \"F\", \"U\", and null. We believe that \"U\" means unknown. So we want to replace missing values with 'U' for this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|DemClusterGroup|count|\n",
      "+---------------+-----+\n",
      "|              F| 3949|\n",
      "|        unknown|  674|\n",
      "|              E| 2607|\n",
      "|              B| 4144|\n",
      "|              U|   54|\n",
      "|              D| 4378|\n",
      "|              C| 4566|\n",
      "|              A| 1850|\n",
      "+---------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|DemGender|count|\n",
      "+---------+-----+\n",
      "|        F|12148|\n",
      "|        M| 5815|\n",
      "|        U| 4259|\n",
      "+---------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|    DemReg|count|\n",
      "+----------+-----+\n",
      "|  Scottish| 1368|\n",
      "|   unknown|  466|\n",
      "|South East| 8634|\n",
      "|South West|  691|\n",
      "|  Midlands| 6740|\n",
      "|     North| 4323|\n",
      "+----------+-----+\n",
      "\n",
      "+------------+-----+\n",
      "|    DemTVReg|count|\n",
      "+------------+-----+\n",
      "|      Ulster|  266|\n",
      "|      N East|  785|\n",
      "|      Border|  203|\n",
      "|     unknown|  465|\n",
      "|      S West|  691|\n",
      "|      London| 6189|\n",
      "|   Yorkshire| 1443|\n",
      "|        East| 1649|\n",
      "|      N Scot|  329|\n",
      "|      N West| 2096|\n",
      "|  C Scotland|  836|\n",
      "|    Midlands| 3122|\n",
      "|Wales & West| 1703|\n",
      "|  S & S East| 2445|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for variable in nominal_with_missing:\n",
    "    # if variable is DemGender, we replace the missing values in the column with 'U'\n",
    "    if variable == 'DemGender':\n",
    "        # the method fillna can be used to replace missing values with a new value. \n",
    "        data = data.fillna({variable:'U'})\n",
    "    # for the other variable, we replace the missing values in the column with 'unknown'\n",
    "    else:\n",
    "        data = data.fillna({variable:'unknown'})\n",
    "    # run value_count to verify:\n",
    "    data.groupBy(variable).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Categorical variable encoding:\n",
    "#### We learned that machine learning algorithms cannot deal with categorical features (i.e., features with strings). So, you need to write code to convert the categorical variables with strings into nominal variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+-------------+---------------+----------------+\n",
      "|DemClusterGroupIndexed|DemGenderIndexed|DemRegIndexed|DemTVRegIndexed|PromClassIndexed|\n",
      "+----------------------+----------------+-------------+---------------+----------------+\n",
      "|                   0.0|             2.0|          1.0|            4.0|             2.0|\n",
      "|                   1.0|             2.0|          1.0|            4.0|             2.0|\n",
      "|                   1.0|             0.0|          1.0|            4.0|             0.0|\n",
      "|                   3.0|             1.0|          1.0|            1.0|             1.0|\n",
      "|                   5.0|             0.0|          1.0|            1.0|             1.0|\n",
      "|                   1.0|             2.0|          2.0|            3.0|             3.0|\n",
      "|                   5.0|             0.0|          1.0|            5.0|             1.0|\n",
      "|                   1.0|             1.0|          2.0|            8.0|             1.0|\n",
      "|                   3.0|             1.0|          1.0|            5.0|             0.0|\n",
      "|                   3.0|             0.0|          5.0|           10.0|             2.0|\n",
      "|                   3.0|             0.0|          2.0|            8.0|             2.0|\n",
      "|                   0.0|             1.0|          0.0|            0.0|             0.0|\n",
      "|                   3.0|             0.0|          1.0|            1.0|             0.0|\n",
      "|                   0.0|             0.0|          0.0|            2.0|             1.0|\n",
      "|                   0.0|             0.0|          0.0|            0.0|             1.0|\n",
      "|                   1.0|             1.0|          1.0|            1.0|             0.0|\n",
      "|                   2.0|             0.0|          1.0|            1.0|             2.0|\n",
      "|                   0.0|             0.0|          1.0|            5.0|             1.0|\n",
      "|                   4.0|             0.0|          3.0|            7.0|             1.0|\n",
      "|                   1.0|             2.0|          0.0|            0.0|             1.0|\n",
      "+----------------------+----------------+-------------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "nominal_with_string = ['DemClusterGroup', 'DemGender', 'DemReg', 'DemTVReg', 'PromClass']\n",
    "for variable in nominal_with_string:\n",
    "    indexed_variable = variable+\"Indexed\" # The variable name of the indexed/encoded variable\n",
    "    indexer = StringIndexer(inputCol=variable, outputCol=indexed_variable)\n",
    "    indexer_model = indexer.fit(data)\n",
    "    data = indexer_model.transform(data)\n",
    "data.select('DemClusterGroupIndexed', 'DemGenderIndexed', 'DemRegIndexed', 'DemTVRegIndexed', 'PromClassIndexed').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4. Dummy Coding:\n",
    "#### Now you have converted categorical variables with strings into nominal variables with indexes. You still need to deal with nominal variables with more than 2 categories - You need to do dummy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------+----------------+------------------+-------------------+\n",
      "|DemClusterGroupIndexedVec|DemGenderIndexedVec|DemRegIndexedVec|DemTVRegIndexedVec|PromClassIndexedVec|\n",
      "+-------------------------+-------------------+----------------+------------------+-------------------+\n",
      "|            (7,[0],[1.0])|          (2,[],[])|   (5,[1],[1.0])|    (13,[4],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[1],[1.0])|          (2,[],[])|   (5,[1],[1.0])|    (13,[4],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[1],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[4],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[1],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[5],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[1],[1.0])|          (2,[],[])|   (5,[2],[1.0])|    (13,[3],[1.0])|          (3,[],[])|\n",
      "|            (7,[5],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[5],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[1],[1.0])|      (2,[1],[1.0])|   (5,[2],[1.0])|    (13,[8],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[1],[1.0])|   (5,[1],[1.0])|    (13,[5],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[0],[1.0])|       (5,[],[])|   (13,[10],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[0],[1.0])|   (5,[2],[1.0])|    (13,[8],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[0],[1.0])|      (2,[1],[1.0])|   (5,[0],[1.0])|    (13,[0],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[3],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[0],[1.0])|      (2,[0],[1.0])|   (5,[0],[1.0])|    (13,[2],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[0],[1.0])|      (2,[0],[1.0])|   (5,[0],[1.0])|    (13,[0],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[1],[1.0])|      (2,[1],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[0],[1.0])|\n",
      "|            (7,[2],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[1],[1.0])|      (3,[2],[1.0])|\n",
      "|            (7,[0],[1.0])|      (2,[0],[1.0])|   (5,[1],[1.0])|    (13,[5],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[4],[1.0])|      (2,[0],[1.0])|   (5,[3],[1.0])|    (13,[7],[1.0])|      (3,[1],[1.0])|\n",
      "|            (7,[1],[1.0])|          (2,[],[])|   (5,[0],[1.0])|    (13,[0],[1.0])|      (3,[1],[1.0])|\n",
      "+-------------------------+-------------------+----------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "# a list of nominal variables that needs to be dummy coded.\n",
    "indexed_nominal_variables = ['DemClusterGroupIndexed', 'DemGenderIndexed', 'DemRegIndexed', 'DemTVRegIndexed', 'PromClassIndexed']\n",
    "for variable in indexed_nominal_variables:\n",
    "    dummy_variable = variable+\"Vec\"\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[variable],\n",
    "                                 outputCols=[dummy_variable])\n",
    "    model = encoder.fit(data)\n",
    "    data = model.transform(data)\n",
    "data.select('DemClusterGroupIndexedVec', 'DemGenderIndexedVec', 'DemRegIndexedVec', 'DemTVRegIndexedVec', 'PromClassIndexedVec').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- DemAffl: double (nullable = true)\n",
      " |-- DemAge: double (nullable = true)\n",
      " |-- DemClusterGroup: string (nullable = false)\n",
      " |-- DemGender: string (nullable = false)\n",
      " |-- DemReg: string (nullable = false)\n",
      " |-- DemTVReg: string (nullable = false)\n",
      " |-- PromClass: string (nullable = true)\n",
      " |-- PromSpend: double (nullable = true)\n",
      " |-- PromTime: double (nullable = true)\n",
      " |-- TargetBuy: integer (nullable = true)\n",
      " |-- DemAfflMissing: integer (nullable = false)\n",
      " |-- DemAfflImputed: double (nullable = true)\n",
      " |-- DemAgeMissing: integer (nullable = false)\n",
      " |-- DemAgeImputed: double (nullable = true)\n",
      " |-- PromTimeMissing: integer (nullable = false)\n",
      " |-- PromTimeImputed: double (nullable = true)\n",
      " |-- DemClusterGroupIndexed: double (nullable = false)\n",
      " |-- DemGenderIndexed: double (nullable = false)\n",
      " |-- DemRegIndexed: double (nullable = false)\n",
      " |-- DemTVRegIndexed: double (nullable = false)\n",
      " |-- PromClassIndexed: double (nullable = false)\n",
      " |-- DemClusterGroupIndexedVec: vector (nullable = true)\n",
      " |-- DemGenderIndexedVec: vector (nullable = true)\n",
      " |-- DemRegIndexedVec: vector (nullable = true)\n",
      " |-- DemTVRegIndexedVec: vector (nullable = true)\n",
      " |-- PromClassIndexedVec: vector (nullable = true)\n",
      "\n",
      "['ID', 'DemAffl', 'DemAge', 'DemClusterGroup', 'DemGender', 'DemReg', 'DemTVReg', 'PromClass', 'PromSpend', 'PromTime', 'TargetBuy', 'DemAfflMissing', 'DemAfflImputed', 'DemAgeMissing', 'DemAgeImputed', 'PromTimeMissing', 'PromTimeImputed', 'DemClusterGroupIndexed', 'DemGenderIndexed', 'DemRegIndexed', 'DemTVRegIndexed', 'PromClassIndexed', 'DemClusterGroupIndexedVec', 'DemGenderIndexedVec', 'DemRegIndexedVec', 'DemTVRegIndexedVec', 'PromClassIndexedVec']\n"
     ]
    }
   ],
   "source": [
    "# now let's take a look at the schema of data again\n",
    "data.printSchema()\n",
    "# print all the variable in the dataframe\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Construct the Features Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's create a list that includes all the independent variable we need to use in the machine learing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this independent variable list needs to include: \n",
    "# 1. For the nominal/categorical variable, you only need to include dummy variables you created for the nominal variable. You don't need to include the original and the indexed nominal variables.\n",
    "# 2. For the interval variables, you need to include:\n",
    "#    a. Interval variables that do not have missing values.\n",
    "#    b. For the interval variables that have missing values, you only need to include the imputed variables and the missing value indicators. E.g., for 'DemAffl', you need to include both DemAfflMissing and DemAfflImputed. You don't want to include the original variables (e.g., DemAffl)\n",
    "# Please remember, you also don't want to include the dependent variables TargetBuy in the independent variable list.\n",
    "\n",
    "independent_variables = ['PromSpend','DemAfflMissing', 'DemAfflImputed', 'DemAgeMissing', \n",
    "                         'DemAgeImputed', 'PromTimeMissing', 'PromTimeImputed',\n",
    "                         'DemClusterGroupIndexedVec', 'DemGenderIndexedVec', 'DemRegIndexedVec', \n",
    "                         'DemTVRegIndexedVec', 'PromClassIndexedVec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLlib expects data to be represented in two columns: a features vector and a label column. Now please write code to prepare the features vector using assembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=140, DemAffl=10.0, DemAge=76.0, DemClusterGroup='C', DemGender='U', DemReg='Midlands', DemTVReg='Wales & West', PromClass='Gold', PromSpend=16000.0, PromTime=4.0, TargetBuy=0, DemAfflMissing=0, DemAfflImputed=10.0, DemAgeMissing=0, DemAgeImputed=76.0, PromTimeMissing=0, PromTimeImputed=4.0, DemClusterGroupIndexed=0.0, DemGenderIndexed=2.0, DemRegIndexed=1.0, DemTVRegIndexed=4.0, PromClassIndexed=2.0, DemClusterGroupIndexedVec=SparseVector(7, {0: 1.0}), DemGenderIndexedVec=SparseVector(2, {}), DemRegIndexedVec=SparseVector(5, {1: 1.0}), DemTVRegIndexedVec=SparseVector(13, {4: 1.0}), PromClassIndexedVec=SparseVector(3, {2: 1.0}), features=SparseVector(37, {0: 16000.0, 2: 10.0, 4: 76.0, 6: 4.0, 7: 1.0, 17: 1.0, 25: 1.0, 36: 1.0})),\n",
       " Row(ID=620, DemAffl=4.0, DemAge=49.0, DemClusterGroup='D', DemGender='U', DemReg='Midlands', DemTVReg='Wales & West', PromClass='Gold', PromSpend=6000.0, PromTime=5.0, TargetBuy=0, DemAfflMissing=0, DemAfflImputed=4.0, DemAgeMissing=0, DemAgeImputed=49.0, PromTimeMissing=0, PromTimeImputed=5.0, DemClusterGroupIndexed=1.0, DemGenderIndexed=2.0, DemRegIndexed=1.0, DemTVRegIndexed=4.0, PromClassIndexed=2.0, DemClusterGroupIndexedVec=SparseVector(7, {1: 1.0}), DemGenderIndexedVec=SparseVector(2, {}), DemRegIndexedVec=SparseVector(5, {1: 1.0}), DemTVRegIndexedVec=SparseVector(13, {4: 1.0}), PromClassIndexedVec=SparseVector(3, {2: 1.0}), features=SparseVector(37, {0: 6000.0, 2: 4.0, 4: 49.0, 6: 5.0, 8: 1.0, 17: 1.0, 25: 1.0, 36: 1.0}))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=independent_variables, outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "\n",
    "data.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Training vs. test dataset partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write code to split your data into a training (70%) and a test dataset (30%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Classifcation: Training with Gradient-boosted tree classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Let's use TargetBuy as the dependent variable and do a classification. The algorithm we use is Gradient-boosted tree. Please refer to https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use Gradient-booted tree. The d\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "algo = GBTClassifier(featuresCol='features', labelCol='TargetBuy', maxIter=10) # set maxIter=10 for GBTClassifier \n",
    "model = algo.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Make predictions using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+\n",
      "|TargetBuy|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|        0|       0.0|[0.71628243970119...|\n",
      "|        0|       0.0|[0.90967713245875...|\n",
      "|        0|       0.0|[0.76752926399651...|\n",
      "|        0|       0.0|[0.86080296901729...|\n",
      "|        0|       0.0|[0.87221748814530...|\n",
      "|        0|       0.0|[0.87370078175685...|\n",
      "|        1|       1.0|[0.44258662592787...|\n",
      "|        0|       0.0|[0.75079342331208...|\n",
      "|        0|       0.0|[0.90688853030195...|\n",
      "|        0|       0.0|[0.86182132521572...|\n",
      "|        1|       0.0|[0.75821051963979...|\n",
      "|        0|       0.0|[0.84544236313838...|\n",
      "|        1|       1.0|[0.31877412258060...|\n",
      "|        0|       0.0|[0.91030208179284...|\n",
      "|        1|       1.0|[0.19866174342649...|\n",
      "|        0|       0.0|[0.87221748814530...|\n",
      "|        0|       0.0|[0.92331143898198...|\n",
      "|        0|       0.0|[0.75079342331208...|\n",
      "|        0|       0.0|[0.83986498350339...|\n",
      "|        0|       0.0|[0.92481590001009...|\n",
      "+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make prediction using test data\n",
    "predictions = model.transform(test)\n",
    "predictions.select(['TargetBuy','prediction', 'probability']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In MLlib the default metric used for evaluating classification models is area_under_roc (auc). Please write the code to get the AUC value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8263046324908051"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using spark ml to do model evaluation \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='TargetBuy', metricName='areaUnderROC')\n",
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation with SciKit-Learn\n",
    "### Next, please write code to use scikit-learn to create the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88      5117\n",
      "           1       0.68      0.44      0.53      1616\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6733\n",
      "   macro avg       0.76      0.69      0.71      6733\n",
      "weighted avg       0.80      0.81      0.80      6733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = predictions.select(['TargetBuy']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
