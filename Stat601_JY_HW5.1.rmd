---
title: "Homework 5"
author: "STAT 601"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F,warning=F,echo=F,fig_height=10,fig_width=7,cache = F)
```


*Here we load in the required packages and the "BostonHousing" data that will be used.
```{r, echo=TRUE}
library(mlbench)
library(rpart)
library(partykit)
library(TH.data)
library(ada)
library(adabag)
library(rattle.data)
library(ggplot2)
data(BostonHousing)
names(BostonHousing)
attach(BostonHousing)
```


1. The \textbf{BostonHousing} dataset reported by Harrison and Rubinfeld (1978) is available as data.frame package \textbf{mlbench} (Leisch and Dimitriadou, 2009). The goal here is to predict the median value of owner-occupied homes  (medv variable, in 1000s USD) based on other predictors in the dataset. Use this dataset to do the following 

    a.) Construct a regression tree using rpart(). The following need to be included in your discussion. How many nodes did your tree have? Did you prune the tree? Did it decrease the number of nodes? What is the prediction error (calculate MSE)?  Provide a plot of the predicted vs. observed values. Plot the final tree.
    
```{r}
set.seed(1)
Boston=rpart(medv~., data = BostonHousing)

opt <- which.min(Boston$cptable[,"xerror"])
    
cp = Boston$cptable[opt, "CP"]
Boston.pruned = prune(Boston, cp = cp)
    
cat("Tree: \n")
medv_pred <- predict(Boston, newdata = BostonHousing)
Tree1MSE = mean((BostonHousing$medv - medv_pred)^2)
Tree1MSE
cat("\nTree with Pruning: \n")
medv_pred2 <- predict(Boston.pruned, newdata = BostonHousing)
PrunedTreeMSE = mean((BostonHousing$medv - medv_pred2)^2)
PrunedTreeMSE
    
xlim = range(BostonHousing$medv)
plot(medv_pred ~ medv, data = BostonHousing, xlab = "Observed", ylab = "Predicted", main="Tree", ylim = xlim, xlim = xlim)
abline(a = 0, b = 1)
    
xlim = range(BostonHousing$medv)
plot(medv_pred2 ~ medv, data = BostonHousing, xlab = "Observed", ylab = "Predicted", main="Pruned Tree", ylim = xlim, xlim = xlim)
abline(a = 0, b = 1)
```
*Here we see that the error rate for trees, both pruned and unpruned, are the same and the predicted vs. observed plots do not change. Lets check out the trees below to check for a node difference.*
    
```{r}
plot(as.party(Boston))
plot(as.party(Boston.pruned), tp_args = list(id = FALSE))
```
*Both pruned and unpruned trees have the same number of nodes, which is seven nodes.*

    
    b) Perform bagging with 50 trees. Report the prediction error (MSE). Provide the predicted vs observed plot. 
    
```{r}
set.seed(1)
trees <- vector(mode = "list", length = 50)
n <- nrow(BostonHousing)
bootsamples <- rmultinom(length(trees), n, rep(1, n)/n)
mod <- rpart(medv ~ ., data = BostonHousing,
             control = rpart.control(xval = 0))
for (i in 1:length(trees))
  trees[[i]] <- update(mod, weights = bootsamples[,i])


table(sapply(trees, function(x)
  as.character(x$frame$var[1])))

x<-trees[[1]]

classprob <- matrix(0, nrow = n, ncol = length(trees))

for (i in 1:length(trees)) {
  classprob[,i] <- predict(trees[[i]],
                           newdata = BostonHousing)
  classprob[bootsamples[,i] > 0,i] <- NA
}


avg <- rowMeans(classprob, na.rm = TRUE)


# Calculate mean square error (MSE)
TreeBagMSE <- mean((BostonHousing$medv - avg)^2)
TreeBagMSE

## RP-Boston Housing-predict and plot
xlim <- range(BostonHousing$medv)
plot(avg ~ medv, data = BostonHousing, xlab = "Observed", 
     ylab = "Predicted", ylim = xlim, xlim = xlim)
abline(a = 0, b = 1)
```

*Here we see MSE actually rises compared to the singular trees we previously made.*
    
    c) Use randomForest() function in R to perform bagging. Report the prediction error (MSE). Was it the same as (b)? If they are different what do you think caused it?  Provide a plot of the predicted vs. observed values.
    
    
    
```{r}
set.seed(1)
library(randomForest)
BHmodel_bag = randomForest(medv~., mtry = 13, ntree=50, data = BostonHousing)
medvbagpred=predict(BHmodel_bag, newdata=BostonHousing)

#MSE
RFBagMSE <- mean((BostonHousing$medv-medvbagpred)^2)
RFBagMSE
xlim <- range(BostonHousing$medv)

#ploting
plot(medvbagpred ~ medv, data = BostonHousing, xlab = "Observed", ylab = "Predicted", ylim = xlim, xlim = xlim)
abline(a=0, b=1)

```

*The random forest bagging model is much better in terms of MSE versus the rpart trees (bagged or unbagged). I think this may because of the difference in the way random forest and rpart work. Each random forest tree is itself an ensemble of "weak-learners" that together have strong predictive ability where each tree of rpart is just a tree.*
    
    d) Use randomForest() function in R to perform random forest. Report the prediction error (MSE).  Provide a plot of the predicted vs. observed values.
    
```{r}
set.seed(1)
rf.mod <- randomForest( medv ~ ., data = BostonHousing)
medv_rf_pred <- predict(rf.mod, newdata = BostonHousing)
RFMSE <- mean((BostonHousing$medv - medv_rf_pred)^2)
RFMSE
xlim <- range(BostonHousing$medv)
#plot observed vs predicted
plot(medv_rf_pred ~ medv, data = BostonHousing, xlab = "Observed", ylab = "Predicted", ylim = xlim, xlim = xlim)
abline(a=0, b=1)
```

*Here we see pretty similar performance to the random forest bagged tree in terms of MSE. We need to keep in mind that this is the MSE for data that the model has already seen and does not imply that it would strongly generalize.*
    
    e) Provide a table containing each method and associated MSE. Which method is more accurate?
    
```{r}
tree.df = data.frame("Method" = c("Single Tree (un-pruned)",
                                      "Single Tree (pruned)",
                                      "50 Tree Ensemble",
                                      "Random Forest Bagging",
                                      "Random Forest"),
                         "Mean_Square_Error" = c(Tree1MSE,
                                                 PrunedTreeMSE,
                                                 TreeBagMSE,
                                                 RFBagMSE,
                                                 RFMSE))
print(tree.df)
```
*In my models, with random.seed set to 1, the most accurate model was the random forest bagging model which had the lowest MSE. In fact, random forest bagging was about 8-fold better than all rpart trees (bagged or unbagged) and slightly better than a "out of the box" random forest. However, it may be possible that this random forest model is overfitting the data it was given and may not generalize well to other data, say from another city.*
    
2. Consider the glaucoma data (data = "\textbf{GlaucomaM}", package = "\textbf{TH.data}").

    a) Build a logistic regression model. Note that most of the predictor variables are highly correlated. Hence, a logistic regression model using the whole set of variables will not work here as it is sensitive to correlation.
        \begin{verbatim}
        glac_glm <- glm(Class ~., data = GlaucomaM, family = "binomial")
        #warning messages  -- variable selection needed 
        \end{verbatim}

        The solution is to select variables that seem to be important for predicting the response and using those in the modeling process using GLM. One way to do this is by looking at the relationship between the response variable and predictor variables using graphical or numerical summaries - this tends to be a tedious process. Secondly, we can use a formal variable selection approach. The $step()$ function will do this in R. Using the $step$ function, choose any direction for variable selection and fit logistic regression model. Discuss the model and error rate.
        
        \begin{verbatim}
        #use of step() function in R
        ?step
        glm.step <- step(glac_glm)
        \end{verbatim}
        
        
        Do not print out the summaries of every single model built using variable selection. That will end up being dozens of pages long and not worth reading through. Your discussion needs to include the direction you chose. You may only report on the final model, the summary of that model, and the error rate associated with that model.

    
    
```{r}
library(TH.data)
data(GlaucomaM)
set.seed(1)

lo = glm(Class ~1, data = GlaucomaM, family = "binomial")
up = glm(Class ~., data = GlaucomaM, family = "binomial")
#step1 =step(lo, scope=list(lower=lo, upper=up), direction="forward")
#step2 =step(up, scope=list(lower=lo, upper=up), direction="backward")
#step3 =step(lo, scope = list(upper=up), data=Housing, direction="both")
final = glm(Class~phci+vass+rnf+mdi+mdt+vbrn+emd+as+abrs+hvc,data=GlaucomaM, family="binomial")
summary(final)

prediction1 <- predict(final, GlaucomaM, type = "response")
prediction11 <- ifelse(prediction1 >= .5, 1, 0)
prediction2 <- ifelse(prediction1 == "glaucoma", 1, 0)
mse1 <- mean((prediction11 - prediction2)^2)
mse1
```
  
  *I tried going in forward, reverse, and both directions (seen in .rmd) and settled on the variables used in the final model. The final model has picked 11 variables out of the starting pool which are all significant at p <0.05 or close to it. This is a binary prediction and my model shows an error rate of...*  
    
    b) Build a logistic regression model with K-fold cross validation (k = 10). Report the error rate.
    
```{r}
library(boot)
set.seed(333)
cost=function(r, pi=0)
    mean(abs(r-pi)>0.5)
    
cat("K-fold cross validation (K=10) error rate: \n")
glau.CV10 = cv.glm(GlaucomaM, final, K=10, cost)$delta[1];glau.CV10
```
*We see the 10-fold cross validation is pretty low at 0.13 but that may not represent a clinically feasible level of proficiency.*
    
    c) Find a function (package in R) that can conduct the "adaboost" ensemble modeling. Use it to predict glaucoma and report error rate. Be sure to mention the package you used.
    
```{r}
library(adabag)
set.seed(1)
ind = sample(2, nrow(GlaucomaM), replace = TRUE, prob = c(0.7, 0.3))
train.df = GlaucomaM[ind == 1,]
test.df = GlaucomaM[ind == 2,]
    
final.boost = boosting(final, train.df, control=rpart.control(minsplit=10))
final.b.pred = predict.boosting(final.boost, newdata=test.df)
final.preds = as.numeric(final.b.pred$class)
final.test.obs = ifelse(test.df$Class == "1", 1, 0)
    
final.boostc = boosting(final, GlaucomaM)
final.b.predc = predict.boosting(final.boostc, newdata=GlaucomaM)
final.predsc = as.numeric(final.b.predc$class)
final.test.obsc = ifelse(GlaucomaM$Class == "1", 1, 0)
    
cat("MSE from whole data boosting: \n")
final.mseW = mean((final.predsc - final.test.obsc)^2)
final.mseW
    
cat("\nMSE from train boosting: \n")
boost.mse = mean((final.preds - final.test.obs)^2)
boost.mse
```
```{r}
predicted = predict(final, test.df, type='response')
y = test$default
error.rate <- mean((predicted>0.5 & y==0) | (predicted<0.5 & y==1))
error.rate
```
    
    
    d) Report the error rates based on single tree, bagging and random forest. (A table would be great for this).
    
```{r}
set.seed(seed=1)
model.df = data.frame("Method" = c("Step GLM",
                                      "K-Fold CV",
                                      "Adaptive Boosting: Whole Data",
                                      "Adaptive Boosting"),
                         "Mean_Square_Error" = c(final.mse,
                                                 glau.CV10,
                                                 final.mseW,
                                                 final.mse))
print(model.df)
```
    
    
    e) Write a conclusion comparing the above results (use a table to report models and corresponding error rates). Which one is the best model?

    

    f) From the above analysis, which variables seem to be important in predicting Glaucoma?
    
```{r}
message("Boosting Feature Importance:")
glau.boost$importance
```
    
    